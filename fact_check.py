import logging
from typing import List, Dict, Any, Tuple, Optional
from collections import defaultdict
import re
import json
from concurrent.futures import ThreadPoolExecutor, as_completed
from functools import lru_cache
import tiktoken
from __init__ import model_selection, MAX_TOKENS

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@lru_cache(maxsize=128)
def get_encoding(model: str) -> tiktoken.Encoding:
    """Returns the encoding for the specified model."""
    try:
        return tiktoken.encoding_for_model(model)
    except KeyError:
        logger.warning(f"No specific tokenizer found for {model}. Using cl100k_base as default.")
        return tiktoken.get_encoding("cl100k_base")

def count_tokens(text: str, model: str) -> int:
    """Count the number of tokens in the given text for the specified model."""
    encoding = get_encoding(model)
    return len(encoding.encode(text))

def truncate_to_token_limit(text: str, max_tokens: int, model: str) -> str:
    """Truncate the text to fit within the specified token limit."""
    encoding = get_encoding(model)
    tokens = encoding.encode(text)
    if len(tokens) <= max_tokens:
        return text
    return encoding.decode(tokens[:max_tokens])

class RetrievalBasedKnowledgeBase:
    def __init__(self, context: List[Tuple[str, str]]):
        """
        Initialize the knowledge base with retrieval results.
        
        Args:
        context (List[Tuple[str, str]]): A list of tuples, each containing (title, content)
        """
        self.facts = self._process_retrieval_results(context)
        self._build_index()

    def _process_retrieval_results(self, context: List[Tuple[str, str]]) -> List[Dict[str, Any]]:
        """Process retrieval results into a structured format with autogenerated keywords."""
        processed_facts = []
        for title, content in context:
            keywords = self._extract_keywords(title + " " + content)
            processed_facts.append({
                "title": title,
                "statement": content,
                "category": "",
                "source": "",
                "confidence": 1.0,
                "keywords": keywords
            })
        return processed_facts

    def _extract_keywords(self, text: str, max_keywords: int = 10) -> List[str]:
        """Extract keywords from the text using a simple frequency-based approach."""
        text = re.sub(r'[^\w\s]', '', text.lower())
        words = text.split()
        stop_words = set(['the', 'a', 'an', 'in', 'on', 'at', 'to', 'for', 'of', 'and', 'or', 'but'])
        words = [word for word in words if word not in stop_words]
        word_freq = defaultdict(int)
        for word in words:
            word_freq[word] += 1
        return sorted(word_freq, key=word_freq.get, reverse=True)[:max_keywords]

    def _build_index(self):
        """Build an inverted index for faster retrieval."""
        self.keyword_index = defaultdict(list)
        for i, fact in enumerate(self.facts):
            for keyword in fact["keywords"]:
                self.keyword_index[keyword.lower()].append(i)

    def retrieve_relevant_facts(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """Retrieve relevant facts based on the query."""
        query_words = set(query.lower().split())
        fact_scores = defaultdict(float)
        for word in query_words:
            if word in self.keyword_index:
                for fact_index in self.keyword_index[word]:
                    fact_scores[fact_index] += 1
        sorted_facts = sorted(fact_scores.items(), key=lambda x: x[1], reverse=True)
        return [self.facts[fact_index] for fact_index, _ in sorted_facts[:top_k]]

def extract_claims(summary: str, model: str = "gpt-4o") -> List[str]:
    """Extract key factual claims from the summary."""
    prompt = f"""
    Extract the key factual claims from the following summary. 
    Present each claim as a separate statement:

    Summary:
    {truncate_to_token_limit(summary, 3000, model)}

    Key Claims:
    1.
    """

    messages = [
        {"role": "system", "content": "You are an AI assistant tasked with extracting key factual claims from summaries."},
        {"role": "user", "content": prompt}
    ]

    try:
        response = model_selection(model, messages=messages, max_tokens=1000)
        claims = response.strip().split('\n')
        return [claim.split('. ', 1)[1] if '. ' in claim else claim for claim in claims if claim.strip()]
    except Exception as e:
        logger.error(f"Error extracting claims: {str(e)}")
        return []

def verify_claim(claim: str, relevant_facts: List[Dict[str, Any]], model: str = "gpt-4o") -> Dict[str, str]:
    """Verify a single claim against the provided relevant facts."""
    prompt = f"""
    Verify the following claim against the provided facts:

    Claim: {claim}

    Relevant Facts:
    {json.dumps(relevant_facts, indent=2)}

    Is the claim supported by the facts? Respond in JSON format with the following structure:
    {{
        "verdict": "SUPPORTED/PARTIALLY SUPPORTED/NOT SUPPORTED/UNCERTAIN",
        "explanation": "Brief explanation of the reasoning",
        "confidence": float  # Overall confidence in the verification (0-1)
    }}
    """

    messages = [
        {"role": "system", "content": "You are an AI assistant tasked with verifying claims against known facts."},
        {"role": "user", "content": prompt}
    ]

    try:
        response = model_selection(model, messages=messages, max_tokens=200, output_json=True)
        return json.loads(response)
    except json.JSONDecodeError:
        logger.error("Failed to parse JSON response from the model")
        return {"verdict": "UNCERTAIN", "explanation": "Unable to verify due to an error in response parsing.", "confidence": 0.0}
    except Exception as e:
        logger.error(f"Error verifying claim: {str(e)}")
        return {"verdict": "UNCERTAIN", "explanation": f"Unable to verify due to an error: {str(e)}", "confidence": 0.0}

def fact_check(summary: str, context: List[Tuple[str, str]], model: str = "gpt-4o") -> Optional[str]:
    """Perform fact-checking on the given summary using the provided context."""
    knowledge_base = RetrievalBasedKnowledgeBase(context)
    claims = extract_claims(summary, model)
    verified_claims = []

    with ThreadPoolExecutor(max_workers=5) as executor:
        future_to_claim = {executor.submit(verify_claim, claim, knowledge_base.retrieve_relevant_facts(claim), model): claim for claim in claims}
        for future in as_completed(future_to_claim):
            claim = future_to_claim[future]
            try:
                result = future.result()
                verified_claims.append((claim, result))
            except Exception as e:
                logger.error(f"Error verifying claim '{claim}': {str(e)}")

    verification_results = "\n".join([f"Claim: {claim}\nVerdict: {result['verdict']}\nExplanation: {result['explanation']}\nConfidence: {result['confidence']}" for claim, result in verified_claims])

    fact_check_prompt = f"""
    Original summary:
    {truncate_to_token_limit(summary, 2000, model)}

    Verification results:
    {truncate_to_token_limit(verification_results, 4000, model)}

    Please rewrite the summary, taking into account the verification results. 
    Clearly indicate any claims that were not fully supported or were uncertain. 
    Maintain the overall structure and insights of the original summary where possible.
    The fact-checked summary should not exceed 1000 tokens.

    Fact-checked summary:
    """

    messages = [
        {"role": "system", "content": "You are an AI assistant tasked with generating fact-checked summaries."},
        {"role": "user", "content": fact_check_prompt}
    ]

    try:
        response = model_selection(model, messages=messages, max_tokens=1000)
        fact_checked_summary = response.strip()
        logger.info(f"Fact-checked summary generated. Token count: {count_tokens(fact_checked_summary, model)}")
        return fact_checked_summary
    except Exception as e:
        logger.error(f"Error generating fact-checked summary: {str(e)}")
        return None

def main():
    # Example usage
    summary = """
    The Earth, our home planet, follows an elliptical orbit around the Sun. 
    This journey takes approximately 365.25 days to complete, which is why we have leap years. 
    Interestingly, the Great Wall of China, one of the most impressive man-made structures, 
    is so large that it can be seen from the Moon with the naked eye. 
    The human body, a marvel of nature, consists of exactly 206 bones, which form our skeletal structure.
    """

    context = [
        ("Earth's Orbit", "The Earth orbits the Sun in an elliptical path. This journey takes approximately 365.25 days to complete, which is why we have leap years."),
        ("Water Boiling Point", "Water boils at 100 degrees Celsius (212 degrees Fahrenheit) at sea level. This temperature can vary based on atmospheric pressure."),
        ("Great Wall of China Visibility", "Contrary to popular belief, the Great Wall of China is not visible from space with the naked eye. This myth has been debunked by numerous astronauts."),
        ("Human Skeletal System", "The adult human body typically contains 206 bones. This number can vary slightly due to anatomical variations in some individuals."),
        ("Mona Lisa Painter", "The Mona Lisa, one of the most famous paintings in the world, was created by the Italian Renaissance artist Leonardo da Vinci. It is housed in the Louvre Museum in Paris.")
    ]

    fact_checked_summary = fact_check(summary, context)
    if fact_checked_summary:
        logger.info("Fact-checked summary:")
        logger.info(fact_checked_summary)
        logger.info(f"Token count of fact-checked summary: {count_tokens(fact_checked_summary, 'gpt-4o')}")
    else:
        logger.error("Failed to generate fact-checked summary")

if __name__ == "__main__":
    main()